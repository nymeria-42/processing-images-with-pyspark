{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Configurando Spark"
      ],
      "metadata": {
        "id": "H_3yb2JkMhgo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYWj056OwdN5"
      },
      "outputs": [],
      "source": [
        "## Instalar as dependências\n",
        "\n",
        "### Instalar Java 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "### Realizar o download do Spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
        "\n",
        "### Descompactar o arquivo baixado\n",
        "!tar xf spark-3.4.0-bin-hadoop3.tgz\n",
        "\n",
        "### Instalar findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "## Configurar as variáveis de ambiente\n",
        "### Importar a biblioteca os\n",
        "import os\n",
        "\n",
        "### Definir a variável de ambiente do Java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "### Definir a variável de ambiente do Spark\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\"\n",
        "\n",
        "### Importar e iniciar a biblioteca do findspark\n",
        "import findspark\n",
        "findspark.init('spark-3.4.0-bin-hadoop3')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baixando dataset do kaggle"
      ],
      "metadata": {
        "id": "IPpjc2syMlDp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al2o8szMwfCD"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR03pLBtwhe9",
        "outputId": "8a4dad6b-4d95-4b34-8186-60429a30e799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading labeled-chest-xray-images.zip to /content\n",
            " 99% 1.16G/1.17G [00:15<00:00, 127MB/s]\n",
            "100% 1.17G/1.17G [00:15<00:00, 83.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d tolgadincer/labeled-chest-xray-images\n",
        "! mkdir labeled-chest-xray-images\n",
        "! unzip -q labeled-chest-xray-images -d labeled-chest-xray-images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checando número de cores do colab"
      ],
      "metadata": {
        "id": "HbneJSEQMxeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import cpu_count\n",
        "\n",
        "cpu_count()"
      ],
      "metadata": {
        "id": "TlK2PKcqGxDo",
        "outputId": "03b8a5ec-f2a9-41a6-ed8d-aa6d44d36808",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iniciando Pyspark"
      ],
      "metadata": {
        "id": "HAcCUz36M0FV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY54r_v2wino"
      },
      "outputs": [],
      "source": [
        "# iniciar uma sessão local\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "import io\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import col, expr, pandas_udf, PandasUDFType, udf\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "spark = SparkSession.builder.appName('ChestImages').master('local[*]').config('spark.ui.port', '4050').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Para visualizar interface gráfica do pyspark do colab"
      ],
      "metadata": {
        "id": "EBZPMYXjM24P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "# !unzip ngrok-stable-linux-amd64.zip\n",
        "# !./ngrok authtoken 2QvMFHiPW2gkYlWhsN1KqdNqzKO_5jV9FXU6ue9d9ZHdnz1zv\n",
        "# get_ipython().system_raw('./ngrok http 4050 &')\n",
        "# !sleep 3\n",
        "# !curl -s http://localhost:4040/api/tunnels"
      ],
      "metadata": {
        "id": "pUGwXOpqAZRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processamento dos dados de entrada"
      ],
      "metadata": {
        "id": "0HO97jryNFqJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FbhVWWCxTe4"
      },
      "outputs": [],
      "source": [
        "# read in the files from the mounted storage as binary file\n",
        "path = \"/content/labeled-chest-xray-images/chest_xray\"\n",
        "path_train = path + \"/train\"\n",
        "path_test = path + \"/test\"\n",
        "\n",
        "images_df_train = spark.read.format(\"binaryFile\") \\\n",
        ".option(\"pathGlobFilter\", \"*.jpeg\") \\\n",
        ".option(\"recursiveFileLookup\", \"true\") \\\n",
        ".load(path_train)\n",
        "\n",
        "images_df_test = spark.read.format(\"binaryFile\") \\\n",
        ".option(\"pathGlobFilter\", \"*.jpeg\") \\\n",
        ".option(\"recursiveFileLookup\", \"true\") \\\n",
        ".load(path_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_df_train = images_df_train.withColumn(\"Target\", expr(\"reverse(split(path, '/'))[1]\"))\n",
        "images_df_test = images_df_test.withColumn(\"Target\", expr(\"reverse(split(path, '/'))[1]\"))"
      ],
      "metadata": {
        "id": "-CAAubdgzAP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHqtZm6jxUc6",
        "outputId": "e0f28792-5985-4b45-cd80-7e067a48f451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- path: string (nullable = true)\n",
            " |-- modificationTime: timestamp (nullable = true)\n",
            " |-- length: long (nullable = true)\n",
            " |-- content: binary (nullable = true)\n",
            " |-- Target: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "images_df_train.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_df_train.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzqtUfaa33QD",
        "outputId": "9f50b76f-2351-4570-b1d7-24b66f5da7a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5232"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento do modelo"
      ],
      "metadata": {
        "id": "LPhgSX78NNlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparação"
      ],
      "metadata": {
        "id": "PnV42rUYNbol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet50(include_top=False)\n",
        "\n",
        "bc_model_weights = spark.sparkContext.broadcast(model.get_weights())\n",
        "\n",
        "def model_fn():\n",
        "  \"\"\"\n",
        "  Returns a ResNet50 model with top layer removed and broadcasted pretrained weights.\n",
        "  \"\"\"\n",
        "  model = ResNet50(weights=None, include_top=False)\n",
        "  model.set_weights(bc_model_weights.value)\n",
        "  return model\n",
        "\n",
        "def preprocess(content):\n",
        "  \"\"\"\n",
        "  Preprocesses raw image bytes for prediction.\n",
        "  \"\"\"\n",
        "  img = Image.open(io.BytesIO(content)).resize([64, 64])\n",
        "  img = img.convert('RGB')\n",
        "  arr = img_to_array(img)\n",
        "  return preprocess_input(arr)\n",
        "\n",
        "def featurize_series(model, content_series):\n",
        "  \"\"\"\n",
        "  Featurize a pd.Series of raw images using the input model.\n",
        "  :return: a pd.Series of image features\n",
        "  \"\"\"\n",
        "  input = np.stack(content_series.map(preprocess))\n",
        "  preds = model.predict(input)\n",
        "  # For some layers, output features will be multi-dimensional tensors.\n",
        "  # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
        "  output = [p.flatten() for p in preds]\n",
        "  return pd.Series(output)\n",
        "\n",
        "\n",
        "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
        "def featurize_udf(content_series_iter):\n",
        "  '''\n",
        "  This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
        "  The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
        "\n",
        "  :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
        "                              is a pandas Series of image data.\n",
        "  '''\n",
        "  # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
        "  # for multiple data batches.  This amortizes the overhead of loading big models.\n",
        "  model = model_fn()\n",
        "  for content_series in content_series_iter:\n",
        "    yield featurize_series(model, content_series)"
      ],
      "metadata": {
        "id": "nyT4W2EoWEX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ce7e7b-94de-410b-cfb0-9a2aec6887c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/functions.py:399: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas UDFs on large records (e.g., very large images) can run into Out Of Memory (OOM) errors.\n",
        "# If you hit such errors in the cell below, try reducing the Arrow batch size via `maxRecordsPerBatch`.\n",
        "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"512\")"
      ],
      "metadata": {
        "id": "uLvNBgeVX_tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# featurization on the spark dataframe.\n",
        "features_df_train = images_df_train.select(col(\"path\"), col(\"Target\"),  featurize_udf(\"content\").alias(\"features\"))\n",
        "features_df_test = images_df_test.select(col(\"path\"), col(\"Target\"),  featurize_udf(\"content\").alias(\"features\"))\n",
        "\n",
        "# post processing features column format\n",
        "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
        "\n",
        "features_df_train = features_df_train.select(\n",
        "   col(\"path\"),\n",
        "    list_to_vector_udf(features_df_train[\"features\"]).alias(\"features\"),\n",
        "   col(\"Target\")\n",
        ")\n",
        "features_df_test = features_df_test.select(\n",
        "   col(\"path\"),\n",
        "    list_to_vector_udf(features_df_test[\"features\"]).alias(\"features\"),\n",
        "   col(\"Target\")\n",
        ")\n",
        "\n",
        "df_train =  features_df_train.sample(fraction=1.0, seed=42)\n",
        "df_test =  features_df_test.sample(fraction=1.0, seed=42)"
      ],
      "metadata": {
        "id": "m_M4BZNmYDHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.cache()\n",
        "df_test.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev936oTe6qHJ",
        "outputId": "6b019773-c3c5-46ad-9f7e-48ab222de2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[path: string, features: vector, Target: string]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento em si\n"
      ],
      "metadata": {
        "id": "IvScmQEqmB97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labelIndexer = StringIndexer(inputCol=\"Target\", outputCol=\"indexedTarget\").fit(features_df_train)\n",
        "\n",
        "# concatenate all feature columns into a single one\n",
        "vectorAssembler = VectorAssembler(inputCols=['features'], outputCol=\"featuresModel\")\n",
        "\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.03,\n",
        "                        elasticNetParam=0.5, labelCol=\"indexedTarget\", featuresCol=\"featuresModel\")\n",
        "\n",
        "sparkdn = Pipeline(stages=[labelIndexer,vectorAssembler,lr])"
      ],
      "metadata": {
        "id": "a3xDbD1sNjDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = sparkdn.fit(df_train)"
      ],
      "metadata": {
        "id": "4_A0u7FpiWbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação"
      ],
      "metadata": {
        "id": "GtWHzmwfmLdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.transform(df_test)"
      ],
      "metadata": {
        "id": "YJ569wJISnrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedTarget\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Accuracy = %g\" % (accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmVrsIF3SNwt",
        "outputId": "c22006d8-d61b-4a45-c894-b7ffdb30fc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy = 0.879808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_and_labels = predictions.select(\"indexedTarget\", \"prediction\").orderBy('prediction')\n",
        "\n",
        "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
        "\n",
        "print(metrics.confusionMatrix().toArray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU12YZaffNKg",
        "outputId": "3dcba899-4eb8-43cc-c0c6-5608833ac981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.4.0-bin-hadoop3/python/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[382.  67.]\n",
            " [  8. 167.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referências"
      ],
      "metadata": {
        "id": "yU8bt28qN-Ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referências:\n",
        "- https://docs.databricks.com/applications/machine-learning/preprocess-data/transfer-learning-tensorflow.html\n",
        "- https://github.com/tntn123/spark_transferlearning/blob/main/main.py"
      ],
      "metadata": {
        "id": "D2GCXkRGz5Eo"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "H_3yb2JkMhgo",
        "IPpjc2syMlDp",
        "HbneJSEQMxeP",
        "HAcCUz36M0FV",
        "EBZPMYXjM24P",
        "0HO97jryNFqJ",
        "LPhgSX78NNlN",
        "PnV42rUYNbol",
        "IvScmQEqmB97",
        "GtWHzmwfmLdN",
        "yU8bt28qN-Ir"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}